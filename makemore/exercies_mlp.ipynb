{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e38008f",
   "metadata": {},
   "source": [
    "E01: Tune the hyperparameters of the training to beat my best validation loss of 2.2\n",
    "\n",
    "E02: I was not careful with the intialization of the network in this video. (1) What is the loss you'd get if the predicted probabilities at initialization were perfectly uniform? What loss do we achieve? (2) Can you tune the initialization to get a starting loss that is much more similar to (1)?\n",
    "\n",
    "E03: Read the Bengio et al 2003 paper (link above), implement and try any idea from the paper. Did it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e103ebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a08fa140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open(\"names.txt\", \"r\").read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f80ceba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data sets\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "\n",
    "\n",
    "def build_dataset(words: list):\n",
    "    chunk_size = 3\n",
    "    X, Y = [], []\n",
    "\n",
    "    for word in words:\n",
    "        context = [0] * chunk_size\n",
    "\n",
    "        for ch in word + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "# Split into training (80%) dev/ validation (10%), training (10%)\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b50351a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize network\n",
    "\n",
    "# create character embeddings (27, 30)\n",
    "C = torch.randn((27,10))\n",
    "W1 = torch.randn((30, 300)) # chunk size of 3 chars * 10 dim char emmbedding, 200 neurons\n",
    "b1 = torch.randn(300) # biases\n",
    "W2 = torch.randn((300, 27))\n",
    "b2 = torch.randn(27)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b2d1e40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1397)\n"
     ]
    }
   ],
   "source": [
    "for i in range(50000):\n",
    "    # Forward pass\n",
    "\n",
    "    ix = torch.randint(0, Xtr.shape[0], (200,))\n",
    "    embeddings = C[Xtr[ix]]\n",
    "\n",
    "    h = torch.tanh(embeddings.view(-1, 30) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "\n",
    "    loss = torch.nn.functional.cross_entropy(logits, Ytr[ix])\n",
    "\n",
    "\n",
    "    # Backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # Updata paramaters\n",
    "    step_size = 0.01\n",
    "    for p in parameters:\n",
    "        p.data -= step_size * p.grad\n",
    "\n",
    "print(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "440849ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1599, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss on dev set\n",
    "emb = C[Xdev]\n",
    "h = torch.tanh(emb.view(-1, 30) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = torch.nn.functional.cross_entropy(logits, Ydev)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5181c39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ashani.\n",
      "nehialle.\n",
      "zuyamarkesssev.\n",
      "javeevriichara.\n",
      "amorziah.\n",
      "brixson.\n",
      "ashoni.\n",
      "laya.\n",
      "esrii.\n",
      "alylynn.\n",
      "aurusson.\n",
      "car.\n",
      "mity.\n",
      "ler.\n",
      "caudrielie.\n",
      "graytoneleiah.\n",
      "imayson.\n",
      "khashleelizsonamillarmyla.\n",
      "nai.\n",
      "bastyanne.\n"
     ]
    }
   ],
   "source": [
    "# Sample model\n",
    "chunk_size = 3\n",
    "\n",
    "for i in range(20):\n",
    "\n",
    "    out = []\n",
    "    context = [0] * chunk_size\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
